{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPsO/wn4+EOjIvwRcMECrZ5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ImAli0/ImAli/blob/main/week13/week13.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8zKpUPoI-MvQ"
      },
      "outputs": [],
      "source": [
        "#Using the TextVectorization layer \n",
        "import string \n",
        "class Vectorizer: \n",
        "  def standardize(selt, text): \n",
        "    text = text.lower() \n",
        "    return \"\".join(char for char in text if char not in string.punctuation) \n",
        "  def tokenize(self, text): \n",
        "    text = self.standardize(text) \n",
        "    return text.split() \n",
        "  def make_vocabulary(self, dataset): \n",
        "    self.vocabulary = {\"\": 0, \"[UNK]\": 1} \n",
        "    for text in dataset: \n",
        "      text = self.standardize(text) \n",
        "      tokens = self.tokenize(text) \n",
        "      for token in tokens: \n",
        "        if token not in self.vocabulary: \n",
        "          self.vocabulary[token] = len(self.vocabulary) \n",
        "    self.inverse_vocabulary = dict( (v, k) for k, v in self.vocabulary.items()) \n",
        "  def encode(self, text): \n",
        "    text = self.standardize(text) \n",
        "    tokens = self.tokenize(text) \n",
        "    return [self.vocabulary.get(token, 1) for token in tokens] \n",
        "  def decode(self, int_sequence): \n",
        "    return \" \".join( self.inverse_vocabulary.get(i, \"[UNK]\") for i in int_sequence) \n",
        "vectorizer = Vectorizer() \n",
        "dataset = [ \"I write, erase, rewrite\" \"Erase again, and then\", \"A poppy blooms.\"] \n",
        "vectorizer.make_vocabulary(dataset) \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_sentence = \"I write, rewrite, and still rewrite again\" \n",
        "encoded_sentence = vectorizer.encode(test_sentence) \n",
        "print(encoded_sentence) \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aDD5nmJSAKHZ",
        "outputId": "07ecdc21-e5c7-433d-c4e4-c4e1106929dd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 3, 1, 7, 1, 1, 6]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoded_sentence = vectorizer.decode(encoded_sentence)\n",
        "print(decoded_sentence)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AKNSgejyCeVP",
        "outputId": "8af535d2-1420-49a5-9c07-a052b95aabc5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i write [UNK] and [UNK] [UNK] again\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import TextVectorization \n",
        "text_vectorization = TextVectorization (output_mode=\"int\",\n",
        "\n",
        ")\n"
      ],
      "metadata": {
        "id": "qI4kWApCEoig"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "import tensorflow as tf\n",
        "def custom_standardization_fn(string_tensor):\n",
        "  lowercase_string = tf.strings. lower (string_tensor)\n",
        "  return tf.strings.regex_replace(\n",
        "      lowercase_string, f\"[{re.escape(string.punctuation)}]\", \"\")\n",
        "def custom_split_fn(string_tensor):\n",
        "  return tf.strings.split(string_tensor)\n",
        "text_vectorization=TextVectorization (\n",
        "    output_mode=\"int\",\n",
        "    standardize=custom_standardization_fn,\n",
        "    split=custom_split_fn,)\n"
      ],
      "metadata": {
        "id": "A8ZO29nFE6ZM"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = [\n",
        "\"I write, erase, rewrite\", \"Erase again, and then\", \"A poppy blooms.\",]\n",
        "text_vectorization.adapt(dataset)"
      ],
      "metadata": {
        "id": "xR-eRyFbF1Ys"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Displaying the vocabulary\n",
        "text_vectorization.get_vocabulary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "prOGNL4mGDaB",
        "outputId": "e707f863-4e64-4282-e572-7bc77da34422"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['',\n",
              " '[UNK]',\n",
              " 'erase',\n",
              " 'write',\n",
              " 'then',\n",
              " 'rewrite',\n",
              " 'poppy',\n",
              " 'i',\n",
              " 'blooms',\n",
              " 'and',\n",
              " 'again',\n",
              " 'a']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary = text_vectorization.get_vocabulary()\n",
        "test_sentence = \"I write, rewrite, and still rewrite again\" \n",
        "encoded_sentence = text_vectorization(test_sentence) \n",
        "print(encoded_sentence)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVgzTPcvGjzF",
        "outputId": "abf2b819-5edf-4fa7-fd9e-ea7dd82625e2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([ 7  3  5  9  1  5 10], shape=(7,), dtype=int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inverse_vocab=dict(enumerate (vocabulary))\n",
        "decoded_sentence=\"\".join(inverse_vocab[int(i)] for i in encoded_sentence) \n",
        "print(decoded_sentence)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kvTmqzZBGyfo",
        "outputId": "bcc175f0-f1b2-4ae6-a8cf-4aa410c05f3b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iwriterewriteand[UNK]rewriteagain\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Two approaches for representing groups of words: Sets and sequences\n",
        "#Preparing the IMDB movie reviews data\n",
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz \n",
        "!tar -xf aclImdb_v1.tar.gz\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S7d8vaMaHNqy",
        "outputId": "2af64ae3-35b8-4fad-86ff-d090c9fa9766"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  8974k      0  0:00:09  0:00:09 --:--:-- 16.5M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r aclImdb/train/unsup\n"
      ],
      "metadata": {
        "id": "htKU_-PXHnrK"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat aclImdb/train/pos/4077_10.txt\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oTQtyR6bHt3M",
        "outputId": "473b3e50-f060-4d47-f1cb-f110a06ad9e4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I first saw this back in the early 90s on UK TV, i did like it then but i missed the chance to tape it, many years passed but the film always stuck with me and i lost hope of seeing it TV again, the main thing that stuck with me was the end, the hole castle part really touched me, its easy to watch, has a great story, great music, the list goes on and on, its OK me saying how good it is but everyone will take there own best bits away with them once they have seen it, yes the animation is top notch and beautiful to watch, it does show its age in a very few parts but that has now become part of it beauty, i am so glad it has came out on DVD as it is one of my top 10 films of all time. Buy it or rent it just see it, best viewing is at night alone with drink and food in reach so you don't have to stop the film.<br /><br />Enjoy"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, pathlib, shutil, random\n",
        "\n",
        "base_dir = pathlib.Path(\"aclImdb\")\n",
        "val_dir=base_dir / \"val\"\n",
        "train_dir=base_dir/\"train\"\n",
        "for category in (\"neg\", \"pos\"):\n",
        "  os.makedirs(val_dir / category)\n",
        "  files=os.listdir(train_dir / category) \n",
        "  random. Random(1337). shuffle(files)\n",
        "  num_val_samples = int(0.2*len(files))\n",
        "  val_files =files [-num_val_samples:]\n",
        "  for fname in val_files:\n",
        "    shutil.move(train_dir / category / fname, val_dir/category/ fname)"
      ],
      "metadata": {
        "id": "LjdwpbidHwRa"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras \n",
        "batch_size = 32 \n",
        "\n",
        "train_ds = keras.utils.text_dataset_from_directory( \"aclImdb/train\", batch_size=batch_size \n",
        ") \n",
        "val_ds = keras.utils.text_dataset_from_directory( \"aclImdb/val\", batch_size=batch_size \n",
        ") \n",
        "test_ds = keras.utils.text_dataset_from_directory( \"aclImdb/test\", batch_size=batch_size \n",
        ") \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bmfKQKLCHyAK",
        "outputId": "12d5647c-1e1c-49bd-9f58-9e5d6c70790a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 20000 files belonging to 2 classes.\n",
            "Found 5000 files belonging to 2 classes.\n",
            "Found 25000 files belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Displaying the shapes and dtypes of the first batch \n",
        "for inputs, targets in train_ds: \n",
        "  print(\"inputs.shape:\", inputs.shape) \n",
        "  print(\"inputs.dtype:\", inputs.dtype) \n",
        "  print(\"targets.shape:\", targets.shape) \n",
        "  print(\"targets.dtype:\", targets.dtype) \n",
        "  print(\"inputs[0]:\", inputs[0]) \n",
        "  print(\"targets[0]:\", targets[0]) \n",
        "  break \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1yR-JBXtPBGU",
        "outputId": "70309647-2de8-4afc-a3ef-87835cd26935"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs.shape: (32,)\n",
            "inputs.dtype: <dtype: 'string'>\n",
            "targets.shape: (32,)\n",
            "targets.dtype: <dtype: 'int32'>\n",
            "inputs[0]: tf.Tensor(b'I sat through this movie expecting a thought-provoking, fact-based film. But instead was given some of the least thought out arguments against the Christian faith imaginable. For instance, in an effort to prove that Christianity is inherently violent, the narrator constantly quotes the bible without giving context, and thus altering the meaning of the text. Jesus is quoted as commanding the execution of those who disobey him, when in fact, the quote is from a parable Jesus told, involving a king who is then quoted. Thus the narrator makes it appear as if Jesus says one thing when he is actually telling a story where one of his characters says it. This is dishonesty in a very obvious form. Is this really what Atheism has to offer the world? This film also attempts to use the success of the Passion of the Christ over Jesus Christ: Superstar and The Last Temptation of the Christ as evidence that Christians are bloodthirsty. He makes no mention of the fact that the Passion was the most historically accurate Bible-film to date. He makes no mention of the fact that it was actually the best liked by critics of the bunch. He then edits in a series of violent images from the Passion as if to hammer home his point. Ironically, he makes no mention of The Texas Chainsaw Massacre which came out a few months later and plays violence for entertainment, versus dramatic effect.<br /><br />One thing that really bothered me was his mockery of people who actually knew more about the subject matter than he did. All the Christians he interviewed were average schmoes in the parking lot of Billy Graham\\'s New York Crusade. Atheists he interviewed for the film were notable authors and scholars. He asked the Christians how the Christian movement started, and of course, they said it started with the Holy Spirit coming to the disciples at Pentecost. Which is correct (Acts 2). He then gives the commentary, \"isn\\'t it funny how so few Christians seem to know the origins of their own faith?\" and proceeds to explain that the apostle Paul started Christianity after being stopped on the road to Damascus. The poor chap seems convinced that Acts 9 happens before Acts 2. More deception? Or is this simply ignorance? He also throws around nonsense that Paul didn\\'t believe Jesus was a real person. Are you kidding me? 1 Corinthians 15 describes Jesus death and resurrection being witnessed by people (whom Paul names in the passage) for the Corinthians to question if they are in doubt!<br /><br />There are many many other examples of how full of crap this \\'documentary\\' is. But because I don\\'t have time or patience to go into them all, I\\'ll skip straight to the end. It\\'s obvious throughout the whole movie that the narrator has an emotional vendetta against his upbringing in the church. And the climax interview is HIS CHILDHOOD PRINCIPLE! In a last-ditch attempt to disprove the Christian faith, the narrator tries to make a fool out of someone who gave him a detention as a child. Is this what passes as an intellectual documentary for the Atheist community? Surely there are intelligent Atheist filmmakers out there who can make a documentary that isn\\'t a load of made-up crap passed off as \\'facts\\'.', shape=(), dtype=string)\n",
            "targets[0]: tf.Tensor(0, shape=(), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_vectorization=TextVectorization (max_tokens=20000,output_mode=\"multi_hot\")\n",
        "text_only_train_ds = train_ds.map(lambda x, y: x) \n",
        "text_vectorization.adapt(text_only_train_ds)\n",
        "binary_1gram_train_ds=train_ds.map(lambda x, y: (text_vectorization(x), y),num_parallel_calls=4)\n",
        "binary_1gram_val_ds=val_ds.map(lambda x, y: (text_vectorization(x), y),num_parallel_calls=4)\n",
        "binary_1gram_test_ds=test_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)\n",
        "\n",
        "for inputs, targets in binary_1gram_train_ds: \n",
        "  print(\"inputs.shape:\", inputs.shape)\n",
        "  print(\"inputs.dtype:\", inputs.dtype)\n",
        "  print(\"targets.shape:\", targets.shape)\n",
        "  print(\"targets.dtype:\", targets.dtype)\n",
        "  print(\"inputs[0]:\", inputs[0]) \n",
        "  print(\"targets [0]:\", targets[0])\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pzwsdfOMPRHQ",
        "outputId": "8e376535-7e66-4390-ea86-934304870c25"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs.shape: (32, 20000)\n",
            "inputs.dtype: <dtype: 'float32'>\n",
            "targets.shape: (32,)\n",
            "targets.dtype: <dtype: 'int32'>\n",
            "inputs[0]: tf.Tensor([1. 1. 1. ... 0. 0. 0.], shape=(20000,), dtype=float32)\n",
            "targets [0]: tf.Tensor(0, shape=(), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "def get_model(max_tokens=20000, hidden_dim=16): \n",
        "  inputs = keras.Input(shape=(max_tokens,))\n",
        "  x = layers. Dense (hidden_dim, activation=\"relu\") (inputs) \n",
        "  x = layers. Dropout (0.5)(x)\n",
        "  outputs=layers.Dense (1, activation=\"sigmoid\")(x)\n",
        "  model = keras.Model(inputs, outputs)\n",
        "  model.compile(optimizer=\"rmsprop\",loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "  return model\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UjQ_n-NqP3Uk"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=get_model()\n",
        "model.summary()\n",
        "callbacks = [keras.callbacks. ModelCheckpoint(\"binary_1gram.keras\",save_best_only=True)]\n",
        "model.fit(binary_1gram_train_ds.cache(),validation_data=binary_1gram_val_ds.cache(),epochs=10,callbacks=callbacks)\n",
        "\n",
        "model = keras.models.load_model(\"binary_1gram.keras\")\n",
        "print(f\"Test acc: {model.evaluate(binary_1gram_test_ds)[1]:.3f}\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u6oKZM9NQO6i",
        "outputId": "0f015e72-67f8-44fe-a4fc-2e41aaf9eb13"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 20000)]           0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 16)                320016    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 16)                0         \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 320,033\n",
            "Trainable params: 320,033\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "625/625 [==============================] - 13s 16ms/step - loss: 0.3984 - accuracy: 0.8331 - val_loss: 0.3068 - val_accuracy: 0.8718\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.2650 - accuracy: 0.9024 - val_loss: 0.3030 - val_accuracy: 0.8802\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.2370 - accuracy: 0.9178 - val_loss: 0.3273 - val_accuracy: 0.8770\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2238 - accuracy: 0.9260 - val_loss: 0.3481 - val_accuracy: 0.8730\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.2092 - accuracy: 0.9323 - val_loss: 0.3599 - val_accuracy: 0.8774\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 3s 6ms/step - loss: 0.2105 - accuracy: 0.9346 - val_loss: 0.3671 - val_accuracy: 0.8812\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.2080 - accuracy: 0.9341 - val_loss: 0.3813 - val_accuracy: 0.8820\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.2104 - accuracy: 0.9366 - val_loss: 0.3865 - val_accuracy: 0.8790\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.2009 - accuracy: 0.9352 - val_loss: 0.3938 - val_accuracy: 0.8814\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.1998 - accuracy: 0.9375 - val_loss: 0.4081 - val_accuracy: 0.8822\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 0.2915 - accuracy: 0.8862\n",
            "Test acc: 0.886\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_vectorization=TextVectorization (ngrams=2,max_tokens=20000, output_mode=\"multi_hot\",)"
      ],
      "metadata": {
        "id": "mQESCYgbSFxX"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_vectorization.adapt (text_only_train_ds)\n",
        "binary_2gram_train_ds=train_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4) \n",
        "binary_2gram_val_ds = val_ds.map(lambda x, y: (text_vectorization(x), y),num_parallel_calls=4) \n",
        "binary_2gram_test_ds = test_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)\n",
        "model = get_model()\n",
        "model.summary()\n",
        "callbacks = [keras.callbacks. ModelCheckpoint (\"binary_2gram.keras\",save_best_only=True)]\n",
        "\n",
        "model.fit(binary_2gram_train_ds.cache(),validation_data=binary_2gram_val_ds.cache(),\n",
        "epochs=10,\n",
        "callbacks=callbacks)\n",
        "model=keras.models.load_model(\"binary_2gram.keras\")\n",
        "print(f\"Test acc: {model.evaluate(binary_2gram_test_ds) [1]:.3f}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZpgG_dR4Sglb",
        "outputId": "9e4c44ab-e5e1-4900-9d8c-2415f4614038"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 20000)]           0         \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 16)                320016    \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 16)                0         \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 320,033\n",
            "Trainable params: 320,033\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "625/625 [==============================] - 9s 14ms/step - loss: 0.3672 - accuracy: 0.8474 - val_loss: 0.2883 - val_accuracy: 0.8858\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.2398 - accuracy: 0.9148 - val_loss: 0.2923 - val_accuracy: 0.8900\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.2156 - accuracy: 0.9320 - val_loss: 0.3137 - val_accuracy: 0.8904\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.1949 - accuracy: 0.9403 - val_loss: 0.3224 - val_accuracy: 0.8898\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.1899 - accuracy: 0.9424 - val_loss: 0.3368 - val_accuracy: 0.8926\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.1764 - accuracy: 0.9470 - val_loss: 0.3573 - val_accuracy: 0.8876\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.1776 - accuracy: 0.9473 - val_loss: 0.3683 - val_accuracy: 0.8882\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.1796 - accuracy: 0.9517 - val_loss: 0.3743 - val_accuracy: 0.8864\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.1830 - accuracy: 0.9507 - val_loss: 0.3827 - val_accuracy: 0.8870\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.1775 - accuracy: 0.9510 - val_loss: 0.3913 - val_accuracy: 0.8854\n",
            "782/782 [==============================] - 9s 11ms/step - loss: 0.2756 - accuracy: 0.8916\n",
            "Test acc: 0.892\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_vectorization=TextVectorization(ngrams =2,max_tokens=20000,output_mode=\"count\")\n",
        "\n"
      ],
      "metadata": {
        "id": "m1ncnK9pSpnn"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_vectorization=TextVectorization(ngrams=2,max_tokens=20000,output_mode=\"tf_idf\")"
      ],
      "metadata": {
        "id": "bkc-AHQRTMRc"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_vectorization.adapt (text_only_train_ds)\n",
        "\n",
        "tfidf_2gram_train_ds = train_ds.map( lambda x, y: (text_vectorization(x), y), num_parallel_calls=4) \n",
        "tfidf_2gram_val_ds = val_ds.map( lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)\n",
        "tfidf_2gram_test_ds=test_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)\n",
        "model = get_model()\n",
        "model.summary()\n",
        "callbacks = [keras.callbacks.ModelCheckpoint(\"tfidf_2gram.keras\",save_best_only=True)]\n",
        "model.fit(tfidf_2gram_train_ds.cache(),validation_data=tfidf_2gram_val_ds.cache(),\n",
        "epochs=10,\n",
        "callbacks=callbacks)\n",
        "\n",
        "model=keras.models. load_model(\"tfidf_2gram.keras\")\n",
        "print (f\"Test acc: {model.evaluate(tfidf_2gram_test_ds) [1]: .3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zShlzR6gTM6Q",
        "outputId": "fd3d6505-0550-4a83-c84b-b1b6f93c8fe0"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_3 (InputLayer)        [(None, 20000)]           0         \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 16)                320016    \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 16)                0         \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 320,033\n",
            "Trainable params: 320,033\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "625/625 [==============================] - 10s 15ms/step - loss: 0.5257 - accuracy: 0.7397 - val_loss: 0.3261 - val_accuracy: 0.8756\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.3495 - accuracy: 0.8533 - val_loss: 0.3284 - val_accuracy: 0.8768\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.2958 - accuracy: 0.8795 - val_loss: 0.2971 - val_accuracy: 0.8958\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.2663 - accuracy: 0.8936 - val_loss: 0.3115 - val_accuracy: 0.8864\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.2452 - accuracy: 0.8983 - val_loss: 0.3222 - val_accuracy: 0.8858\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.2351 - accuracy: 0.9032 - val_loss: 0.3583 - val_accuracy: 0.8752\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.2347 - accuracy: 0.9036 - val_loss: 0.3520 - val_accuracy: 0.8816\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.2224 - accuracy: 0.9116 - val_loss: 0.3452 - val_accuracy: 0.8784\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.2172 - accuracy: 0.9104 - val_loss: 0.3544 - val_accuracy: 0.8814\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2144 - accuracy: 0.9112 - val_loss: 0.3573 - val_accuracy: 0.8854\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 0.3022 - accuracy: 0.8908\n",
            "Test acc:  0.891\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs=keras. Input (shape=(1,), dtype=\"string\")\n",
        "processed_inputs = text_vectorization (inputs)\n",
        "outputs = model (processed_inputs)\n",
        "inference_model = keras.Model (inputs, outputs)\n",
        "\n"
      ],
      "metadata": {
        "id": "SCORW1I9TWdo"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "raw_text_data = tf.convert_to_tensor([[\"That was an excellent movie, I loved it.\"]\n",
        "])\n",
        "\n",
        "predictions = inference_model(raw_text_data)\n",
        "print (f\" {float(predictions[0]* 100):.2f} percent positive\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29cGMNYqT3tz",
        "outputId": "e57a97a4-a243-4f5f-ef56-86c0b64d798d"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 99.07 percent positive\n"
          ]
        }
      ]
    }
  ]
}